{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bc0488-5fe0-4b47-9ff1-21dc931c9351",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3.3 Grouping and Aggregating Data\n",
    "\n",
    "The ability to group and aggregate data is one of the most powerful features of Pandas. Using the aggregation functionality allows analysts to quickly compute summary statistics over their data set at varying levels of specificity that they can choose. Aggregation in Pandas includes calculations such as `count`, `nunique` (distinct count), `sum`, `mean`, `median`, `mode`, `max`, `min` and `std` (standard deviation), among others.\n",
    "\n",
    "Grouping and aggregating in Pandas is based on the same principles as grouping and aggregation in SQL. In SQL, you would SELECT an aggregate function and then GROUP BY all of the non-aggregate columns. In Pandas, we just have to worry about which columns to group by and which columns to aggregate.\n",
    "\n",
    "Groups and aggregations are almost always used together. They allow the data analyst to \"drill down\" into sub-categories of the data and view trends between and within groups.\n",
    "\n",
    "### About the data\n",
    "\n",
    "The data used in this notebook shows information about passengers on the *Titanic* cruiseliner, a ship which set out from Southampton, U.K. to sail across the Atlantic ocean and which tragically sank upon collision with an iceberg. The dataset contains information about each passenger's passenger class, name, sex, age, siblings, parents/children, ticket number, ticket fare, cabin number, and the embarked location. It also contains information about each passenger's survival status. This data set is extremely popular among data scientists and will facilitate demonstrations of Pandas concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f22b5d-2632-4bde-898b-a348bb2f14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb05bb-6323-4add-9ec9-a60e15ba3073",
   "metadata": {},
   "source": [
    "### Groups\n",
    "In order to perform aggregation, Pandas needs to know how to group the data. In SQL, we used the GROUP BY clause to explicitly tell the query how to group the aggregations. In this *Titanic* data set, for example, we could group by passenger class (Pclass), where the person embarked (Embarked), or whether or not the person survived (Survived). In Pandas, we can actually perform an aggregation on the entire dataframe at once without specifying a group, and Pandas will just assume that the whole dataset is one big group.\n",
    "\n",
    "However, it is also important to specify how the rows should be grouped when they are aggregated. Below you will see how to aggregate on a dataframe without grouping AND will also see how to create groups that can then be aggregated.\n",
    "\n",
    "### Using aggregate functions on an entire dataframe\n",
    "Previously, we applied aggregate functions to an entire dataframe or Series object by using methods such as `.max()`, `.mean()`, and `.sum()`. As stated before, all aggregate functions need to be grouped before they can be processed. So, what are the groups when we use an aggregate function on an entire dataframe?\n",
    "\n",
    "Simply enough, the aggregate function treats the entire dataframe as one big group. Thus, there is no need to create groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f712dbc-dd9d-4c49-b752-05c046ed7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bfc04-766c-4d1b-9a1d-9d407f5694aa",
   "metadata": {},
   "source": [
    "The code above `df.sum()` returned a Series object with all of the rows added/concatenated together. Some of the information is useful (like knowing that 342 people survived the Titanic), but other information is not as useful (like the total age being 21,205... what does that even mean?). Other information is completely useless (such as the total \"Sex\" being \"malefemalefemalefemalemale...\").\n",
    "\n",
    "In this way, we see that aggregating across an entire dataframe *can* be useful when you don't know much about your data and need quick information, but it's probably better to choose which columns to aggregate and which groups to create. This will allow you to obtain detailed information about specific groups. Grouping also allows you to control how aggregations are applied multiple groups at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b1b6e-afcf-4a73-b3d2-ef3e5409c579",
   "metadata": {},
   "source": [
    "### Creating a group by object\n",
    "Before applying an aggregate function to groups in a dataframe, Pandas first requires the creation of a `groupby` object. This can be done by using the `.groupby()` method and specifying a list of columns to group by. If there is only one column to be grouped, it can either be passed inside a list or by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88975f48-9fcd-48c4-b42c-56b01f13dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed1e15-f696-43be-ac0a-1a97940f111a",
   "metadata": {},
   "source": [
    "Notice that running the line above (the `.groupby()` method) didn't return back a dataframe; it returned a `DataFrameGroupBy` object. This object has partitioned out each of the rows of the dataframe into distinct groups (which we defined), but doesn't know exactly how they need to be aggregated just yet.\n",
    "\n",
    "Next, we will learn how to use aggregate functions on the `DataFrameGroupBy` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682b3d4-9a81-4aba-818c-57d5f9201822",
   "metadata": {},
   "source": [
    "### Aggregating the groups\n",
    "Aggregate functions can be run directly on the `DataFrameGroupBy` object. There are two ways to do so. The first way to run aggregations on groups is by using a dataframe method on a `DataFrameGroupBy` object (ie. `.sum()`). The second way is to use the `.agg()` method, passing in a dictionary of aggregations to perform.\n",
    "\n",
    "Personally, I prefer to use the `.agg()` method because it allows me to use mutliple aggregations at a time, but will also allow me to perform just a single aggregation too.\n",
    "\n",
    "#### Method 1: Dataframe methods\n",
    "\n",
    "The first way to use an aggregate function with `.groupby()` is by using a a built-in dataframe method to compute a single calculation across the dataframe. These built in aggregation methods include `.count()`, `.nunique()`, `.mean()`, `.median()`, and `.std()`.\n",
    "\n",
    "Note that the `.mode()` method can only be applied to a Series or dataframe, not to a `DataFrameGroupBy` object. Additionally, the `.max()` and `.min()` methods can only be applied to dataframes, Series, and `DataFrameGroupBy` objects that contain exclusively numerical data.\n",
    "\n",
    "##### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57510fa3-635e-4b21-bed1-d53ffd597f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06741561-8b32-457a-ab80-9baedfa4ba4c",
   "metadata": {},
   "source": [
    "##### Count distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66a0ce-a5c8-4d5a-b61b-560322458ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64259fd1-665e-4529-895e-600d14e5ca89",
   "metadata": {},
   "source": [
    "##### Mean (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a52e57-b90e-4fdf-9ffd-0b3196f9cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197429f0-539f-43bb-9498-03d3b8d2b83b",
   "metadata": {},
   "source": [
    "##### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc93a3-173d-4eb3-a678-9c3d0a33662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2eef9-a0e4-4c0b-b2a5-7d2c0140373a",
   "metadata": {},
   "source": [
    "##### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370eb10e-1d01-4cd3-a988-adf52faee719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b43714-88ba-4a5b-a5fa-94c625191d36",
   "metadata": {},
   "source": [
    "##### Max\n",
    "Note that the `.max()` method can only be applied to dataframes, Series, and `DataFrameGroupBy` objects that contain exclusively numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef39b80-d5ed-400e-8fed-cac6205a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Pclass', 'Fare']].groupby(['Pclass']).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10c672-8051-4675-9599-07da964ee662",
   "metadata": {},
   "source": [
    "##### Min\n",
    "Note that the `.min()` method can only be applied to dataframes, Series, and `DataFrameGroupBy` objects that contain exclusively numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c66916-07c2-4085-b01d-a51afa9eaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Pclass', 'Fare']].groupby(['Pclass']).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad3c67-ac22-4f3d-ae41-15223cce7e6e",
   "metadata": {},
   "source": [
    "#### Method 2: The `.agg()` method\n",
    "\n",
    "`DataFrameGroupBy` objects can also have many aggregate functions applied to them at once. This can be done by applying the `.agg()` method, which is unique to `DataFrameGroupBy` objects.\n",
    "\n",
    "The `.agg()` method accepts a dictionary where each key is the field to be aggregated and the value attached to that key is the aggregation to be applied.\n",
    "\n",
    "You can apply the following aggregate functions to the groupby:\n",
    "\n",
    "| Key-word      | Description |\n",
    "| ----------- | ----------- |\n",
    "| `count`      | Count       |\n",
    "| `sum`   | Sum        |\n",
    "| `mean`      | Average (mean)       |\n",
    "| `median`   | Median        |\n",
    "| `nunique`      | Count Distinct       |\n",
    "| `min`   | Minimum (only works on numerical data)       |\n",
    "| `max`      | Maximum (only works on numerical data)    |\n",
    "| `std`      | Standard Deviation       |\n",
    "| `var`   | Variance        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f650807-9306-427a-beeb-90251bf1d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).agg({'Fare': 'median', 'Survived': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58e496-7707-43bb-869b-e86f9afcb632",
   "metadata": {},
   "source": [
    "You can also switch out each value for a list of aggregations to compute for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b20886-3c3a-41f3-b300-cba60c81e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass']).agg({'Fare': ['median', 'mean'], 'Survived': ['mean', 'nunique']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d0382-b96a-4fb7-a1c0-5c9929ef45f0",
   "metadata": {},
   "source": [
    "### The `MultiIndex`\n",
    "Observe the levels of column names generated by the above code. In order to organize the aggregated table (because there are two columns called \"mean\"), Pandas automatically created a `MultiIndex` to \"better\" organize things. You can see how the columns are organized by looking at the `.columns` property of the dataframe.\n",
    "\n",
    "Pandas creates a MultiIndex to clarify which aggregations were performed on each column.\n",
    "\n",
    "Below, we create a new dataframe `grouped_df` that contains data aggregated by Pclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5e2e7-8708-4e9b-b9dd-321b7cc8d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['Pclass']).agg({'Fare': ['median', 'mean'], 'Survived': ['mean', 'nunique']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d43d7b-1983-4cd7-a960-1cbc194cbf02",
   "metadata": {},
   "source": [
    "Now, let's look at the columns of this aggregated dataframe. Notice that the `MultiIndex` is composed of a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ca972-b88a-4ffe-8ee6-263c30a47743",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.columns # The columns are not a list of strings anymore-- they are a list of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2172a-a780-4d43-945b-fd7d40e6e7c7",
   "metadata": {},
   "source": [
    "The `MultiIndex` *can* be useful, but sometimes its just a hassle to deal with. As long as only one aggregation is performed per column in the `.agg()` method, we can tell the `.groupby()` to not create a MultiIndex by passing in the parameter `as_index=False`. This means that the original column name will be assigned to the column instead of the aggregation term.\n",
    "\n",
    "I personally prefer to use a `as_index=False` whenever possible. It makes things easier for me to understand and makes it easier to work with the results of the aggregation. The downside is that I can't tell what aggregations were performed without looking at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6d34c-260a-4733-a527-73cae30f314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Pclass'], as_index=False).agg({'Fare': 'median', 'Survived': 'mean'}) # Each column is only aggregated once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ec07c-6c24-41e5-be58-3e0465f7c173",
   "metadata": {},
   "source": [
    "When more than one aggregation is performed on a single column, however, the `MultiIndex` is necessary in case the same aggregation is performed on several different columns. You can still pass in `as_index=False` but **shouldn't** because it will disrupt the formatting of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186ebd1-89ac-48ba-b35a-669468749065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_index was not passed in. Pclass is the named row index, meaning data is easily extracted by Pclass\n",
    "df.groupby(['Pclass']).agg({'Fare': ['median', 'mean'], 'Survived': ['mean', 'nunique']}) # More than one aggregation applied to one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360f9b7-b9f1-4243-ae2b-48c2c76f3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_index=False was passed in and the row index was reset. Now, a MultiIndex must be used to extract data\n",
    "df.groupby(['Pclass'], as_index=False).agg({'Fare': ['median', 'mean'], 'Survived': ['mean', 'nunique']}) # More than one aggregation applied to one or more columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505bfb2-0262-4dd8-9ed1-c2d5a87df23a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Accessing values in an aggregated table with a `MultiIndex`\n",
    "\n",
    "It may be intimidating to know how to work with data in a dataframe with a `MultiIndex`. However, it's fairly simple. You can access the highest level column by passing in the column name as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0d944-4edc-4c4b-906b-6b77a68b1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the whole dataframe\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df880b-6cbd-4c43-8b2f-f99d2e1a3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the column 'Fare', which is itself a dataframe composed of two columns\n",
    "grouped_df['Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c493c-5637-4de2-8c5b-7ecce26a3957",
   "metadata": {},
   "source": [
    "You can then simply add another column inside the brackets to dive deeper into the levels. This is a single column and thus, a Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0072c-a069-44b4-8825-7c7137e7e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the column 'median' from inside the column 'Fare'\n",
    "grouped_df['Fare', 'median']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01b30f-75b1-4ead-ae4c-8340618386fa",
   "metadata": {},
   "source": [
    "If you are still confused by the `MultiIndex`, don't worry! This is a difficult concept to grasp and is something that you will only see when aggregating data. In this page, I just want you to get your first exposure to this concept so that you will be able to recognize it later on. You don't need to be an expert on using the `MultiIndex` just yet, but should at least be able to say *why* it is used in aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f57a4-075a-4ded-ad53-926e381ca3d3",
   "metadata": {},
   "source": [
    "### The `.value_counts()` method\n",
    "\n",
    "We previously looked at the `.value_counts()` method for Series and dataframes. This method counts up the number of occurrences of values in the Series or dataframes and can also be used with `DataFrameGroupBy` objects. This can be especially useful for filtering data and normalizing the results. We can use the `.value_counts()` method on a Series extracted from the `DataFrameGroupBy` object.\n",
    "\n",
    "In the code below, we group by \"Pclass\" and then look at just the \"Embarked\" column, counting up how many times each value of \"Embarked\" occurs among each \"Pclass\". Thus, the results below show, for each Pclass, how many people embarked in Southampton, Cherbourg, and Queenstown. For example, we can see below that the majority of people who embarked in Queenstown (\"Q\") were third class passengers (Pclass was 3). In other words, first class passengers (Pclass=1) had 2 embarkments in Queenstown, second class passengers (Pclass=2) had 3 embarkments in Queenstown, and third class passengers (Pclass=3) had 72 embarkments in Queenstown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96092c89-c2dc-4639-a988-d270b294856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Pclass\")[\"Embarked\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfef10-b5db-4d75-98ef-1d0e25cea8df",
   "metadata": {},
   "source": [
    "#### Looking at MultiIndexed rows\n",
    "\n",
    "Note that above, a Series was returned. Notice also the reappearance of the `MultiIndex`, this time on the rows. This allows us to look more closely at the number of people who embarked at each location between passenger classes. The `MultiIndex` is necessary for separating out each level of aggregation, meaning that it lets us count up each embarked location for each passenger class.\n",
    "\n",
    "We can access the outer row index (in this case, \"Pclass\") by using the `.loc` property. This is exactly how we would access any row in a normal dataframe. Using `.loc` could be useful for drilling down into the number of people embarked in a specific passenger class.\n",
    "\n",
    "In the code below, we group by \"Pclass\" and then count up the number of times each \"Embarked\" value took place across each passenger class. We then use `.loc` to get just the row where Pclass (the outermost named row index) is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9115751-68a3-4faa-924c-8e4a0e73f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Pclass\")[\"Embarked\"].value_counts().loc[1] # The number 1 is the row index name. Note that .iloc[1] would get the index location and would directly access the second value in the Series and would not have the same outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa5547-8abd-4122-a5d1-05f273dd8a70",
   "metadata": {},
   "source": [
    "Again, you do not need to be an expert at using the `MultiIndex` to succeed in this class. For now, it is important that you recognize it and know why it would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8775d-3e5b-4ad7-aba1-b1f4ac2ef73f",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "Pass in `normalize=True` to the `.value_counts()` method to normalize the values (ie. express them as a percentage of the total). Each total is calculated at the deepest level of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c288bc-30ec-4431-8e3d-480defecd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Pclass\")[\"Embarked\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06ae15-504b-4d1a-a5df-cdcfca32147d",
   "metadata": {},
   "source": [
    "If you wanted to normalize the value counts across the Pclass dimension, you could calculate it manually. *I haven't yet found a way to specify the level of detail in the `.value_counts()` method, but you could use something like the following to achieve normalization across two dimensions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ef9c2-cacc-405c-bad7-7509cdbb5189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_observations = df['Pclass'].count().sum() # get the total number of non-null values in the 'Pclass' column\n",
    "df.groupby(\"Pclass\")[\"Embarked\"].value_counts() / number_of_observations # Get the value_counts Series and divide each row by number_of_observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
