{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2dd5fee-d3ba-463b-acfa-24858edffc65",
   "metadata": {},
   "source": [
    "# 3.4 Cleaning Data\n",
    "\n",
    "Cleaning data is one of pandas' primary functions. It is often said that data scientists spend anywhere from 40-50% of their time cleaning data, a necessary step before analysis can occur.\n",
    "\n",
    "### What is \"dirty data\"?\n",
    "\n",
    "Many rows of data are often useless to the overall data analysis because they (1) *contain null values*, (2) *do not match the datatype* of other values in the column, or (3) because the *format is inconsistent with other, similar values*. Data is called \"dirty\" when values like this appear in the dataset.\n",
    "\n",
    "##### Contains null values\n",
    "Data containing null values can be problematic because they can skew analysis. If you have a dataset that is 100 rows long, for example, but only three of those rows have a value for the \"Age\" column, the `.mean()` calculation will likely not be representative of the entire population. There are several ways to deal with null values, although we will not go over them extensively in this notebook. Many data scientists use **imputation** (calculate and assign a value to each row) to fill in the blanks, and others simply **drop** the null rows or the entire column. The decision of \"what to do\" really depends on the situation and the context of the data.\n",
    "\n",
    "##### Data type doesn't match\n",
    "If the data types in a column don't match, there could be problems with analysis. For example, in a dataset that records the number of bathrooms in a house, one row could list a house with an integer (1) and another house with a string (\"two\"). The values `1` and `\"two\"` are different data types. They either both need to be converted to be the same data type, or one needs to be removed from the data set.\n",
    "\n",
    "##### Inconsistent format\n",
    "Imagine that you are performing aggregations on a dataset and grouping by U.S. state. However, some of the values for \"state\" are recorded using the two-letter code (ie. UT) and others are recorded with the full name of the state (Utah). Pandas won't know that these two places should be grouped together and so each one will be its own group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485f691-2b32-4624-8c69-84ef17d3fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec98aa1-d527-4aa4-a917-684a8ece2292",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data contains null values\n",
    "Pandas has several methods for looking at null values. Note that in Pandas, null values are called `NaN` (not a number).\n",
    "\n",
    "##### `.isna()`\n",
    "The `.isna()` method can be applied to a Series or a dataframe and will return a boolean value describing if the value was missing (null) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3989950-0a3c-4ed7-b854-bc85f2a7b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a51969-a6d6-4262-8348-f76409bc938d",
   "metadata": {},
   "source": [
    "We can count up the number of null values in each column by adding the `.sum()` aggregation method at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537513fb-5da8-404c-8379-755915b84cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d61093-6b3e-4965-8217-68144cc89272",
   "metadata": {},
   "source": [
    "When it comes to null values, there are several things we could do. If the data is categorical and there are few null values, we might simply assign the most common category to each null row. If the data is quantitative, we might assign each null value the average or median of the column. This is called *imputation*.\n",
    "\n",
    "Another approach would be to drop rows with null values. This would reduce the overall number of observations but could remove some inacurracies that could be produced by imputation.\n",
    "\n",
    "Let's **impute** the two missing values for the \"Embarked\" column by assigning those two values the most commonly occuring value. We can do this by using the `.fillna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923fa05-5166-45a6-9aeb-c8ccff49c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_embarked = df['Embarked'].mode().iloc[0] # The `.mode()` method returns a Series-- get the first and only item in the Series\n",
    "df['Embarked'].fillna(most_common_embarked, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa25ac5-d171-48a2-a582-7642926ea824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fb621-6718-4696-914e-412d1f233b5f",
   "metadata": {},
   "source": [
    "It looks like there are 177 null values in the \"Age\" column. That seems like a lot to impute since there are only 800 ish rows in total. Let's **drop** them instead, with the `.dropna()` method. This method accepts a parameter `how`, which can be \"any\" or \"all\", meaning that it will drop the row if *any* of the columns in the subset are null or only if *all* columns in the subset are null. The `subset` parameter requires a list of columns to look for null values in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac74db-ad14-49df-b3b2-6058ae7b29ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(how='any', subset=['Age'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd6e9a-8858-4437-bb71-02affd272a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce999a-941a-48bf-85d5-8c2762af9d94",
   "metadata": {},
   "source": [
    "### Data type doesn't match\n",
    "\n",
    "We can use the `.dtypes` property to see the data types of each column as automatically interpreted by Pandas. Note that data type `object` usually means \"string\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521860bd-95be-48bf-89b3-bac3bb15478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfe8ad-5efa-450f-bfae-f24687de63d9",
   "metadata": {},
   "source": [
    "Observe that the \"Age\" column has the data type `float64`. That might be intentional, but if we wanted to group passengers by their age we might want them to express their age as an integer instead. \n",
    "\n",
    "We can convert the values in the column to an integer type using the `.astype()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b595141-05d1-4359-ba34-948893b01db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d39db2-a1ab-4eb0-8d53-b31ae9b9a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38231d05-6523-4886-b7cc-c7b4fcfaebb8",
   "metadata": {},
   "source": [
    "### Inconsistent format\n",
    "\n",
    "In this data set, it appears that tickets have no standardization. Some of them start with numbers and some of them start with letters. All of them, however, seem to have a string of five or six numbers at the end. We can trim off the leading letters and preserve just the string of numbers by applying a function to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c48a11-5066-4af1-a901-f1ae86091eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bee5d-7256-4c6c-a479-b65279b9246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTicketNumber(rawTicket):\n",
    "    ticketParts = rawTicket.split(\" \") # split the ticket by its spaces into a list\n",
    "    if len(ticketParts) == 1:          # If the list only has one value (ie. no spaces in ticket, so wasn't split)\n",
    "        return ticketParts[0]          # Return the first and only value\n",
    "    else:                              # Otherwise return the last item in list (gets number without preceding text)\n",
    "        return ticketParts[-1]\n",
    "    \n",
    "df['TicketNumber'] = df['Ticket'].apply(getTicketNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a3f83c-eaa1-407f-810e-6177621e8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e5e84-49fe-4268-8314-3bc875713cc7",
   "metadata": {},
   "source": [
    "There are other situations in which your data may need cleaning, and there are many more Pandas methods and parameters available to use. The more familiar you become with Pandas, the better you will be able to clean your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
